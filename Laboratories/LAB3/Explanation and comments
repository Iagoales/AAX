First of all we opened the pkl file and printed some of its data to see the number of attributes the dataset has, then for the first exercise we decided to do a plot of the ten most active APs to pick the ones we will be using for this project.
After plotting them we find that the APs more interesting and that we believe that will gave us the better results to extract conclussions for our model were: ["7-1048", "7-1079", "7-1057"].
Then we perform an ADF test where our ADF test results provide strong statistical evidence that all three time series are stationary. This means they do not have persistent trends or changing variance over time, and are well-suited for time series modeling without further transformation.
The metrics used in the ADF test—especially the test statistic, p-value, and critical values—offer a robust and widely accepted framework for making these determinations in time series analysis
The conclussion is that all of them are stacionary as the ADF statistic (more negative values indicate stronger evidence against H₀), the p-value (where p < 0.05 suggests stationarity), and critical values (for comparison at 1%, 5%, and 10% significance levels).
Where the ADF test evaluates the null hypothesis (H₀) that a series is non-stationary.

Then for the second exercise this code prepares time series data from a DataFrame for machine learning models, particularly those that require sequential input, such as recurrent neural networks.
It focuses on filtering data for a specific Access Point (AP) by its ID, then processes the data to create input-output sequences using a sliding window approach.
The function prepare_ap_data_for_model first filters and sorts the data for the chosen AP, removes any rows with missing values, and splits the data into training and testing sets. It then standardizes the feature columns using StandardScaler to ensure all features have zero mean and unit variance, which is crucial for many machine learning algorithms.
The core of the process is the create_time_series_sequences function, which generates overlapping sequences of features (inputs) and corresponding target values (outputs) for both training and testing sets.
The final output consists of train/test sequences and the scaler, ready for use in time series forecasting models.

For the third exercsie we chose the 70% training, 15% validation and 15% test split as for time series modeling, a time-based split is especially beneficial, such as the 70/15/15 method for training, validation, and testing.
A chronological split preserves the order of occurrences, unlike random splits that break the temporal order. This is crucial because with time series data, past values affect future results.  
This approach provides a realistic assessment of model performance by closely resembling real-world forecasting scenarios, in which models are trained on historical data and utilized to predict future events. 
The test set, which consists of the most recent data, provides an objective evaluation of the model's performance in production, while the validation set, which comes after the training set in time, allows hyperparameters to be adjusted to maximize generalization for subsequent data.

FOr the fourth exercise the code defines TimeSeriesLSTM, a unique PyTorch neural network class intended for time series prediction applications.
A Long Short-Term Memory (LSTM) layer, a specific kind of recurrent neural network (RNN) that works especially well with sequential data, is used in the model.  
In order to capture temporal dependencies and patterns over time, the LSTM layer processes input sequences (such as sliding windows of time series information).  
Following input processing by the LSTM, the model generates the final prediction by passing the output from the previous time step—which represents the learnt summary of the complete input sequence—through a fully connected (linear) layer.
LSTMs are highly beneficial for time series analysis because they can learn long-term dependencies and relationships in sequential data, overcoming the vanishing gradient problem that limits traditional RNNs.
Their internal memory cells allow them to retain information over many time steps, making them effective for tasks like forecasting, anomaly detection, and sequence classification where past context is crucial for accurate predictions

In the exercise 5 we implemented a centralized training approach for predicting a target variable (such as network usage or signal strength) for different Access Points (APs) using a Random Forest regression model (is highly beneficial for machine learning tasks due to several key strengths. First, it is an ensemble algorithm that builds multiple decision trees on different random subsets of the data and features, then combines their predictions, which significantly improves accuracy and stability compared to a single decision tree. This ensemble approach also makes Random Forest robust to overfitting, as averaging the predictions of many trees reduces the impact of noise and outliers in the training data).
The main idea is to train a model on data from all APs except one (the target AP), then test the model’s performance on the excluded AP. This simulates a real-world scenario where you want to generalize predictions to a new, unseen AP.
The results for the Random Forest models indicate significant issues with predictive accuracy and generalization across Access Points (APs). The validation and test metrics—Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE)—are all extremely high, especially on the test sets. For example, MAPE values reach into the thousands of percent, and MAE values are very large relative to typical scales for regression tasks.
These high errors suggest that the model is struggling to generalize from the training data (which excludes the target AP) to the test data (the target AP itself). 
This could be due to substantial differences in the patterns or distributions between APs, meaning that what the model learns from other APs does not translate well to the held-out AP.
In time series or spatial settings, such heterogeneity is common and can severely limit the effectiveness of centralized models trained in this way.

**EXTRA**
The updated code for time series prediction on network data offers a number of noteworthy enhancements that increase the modeling process's interpretability and dependability.  The strong feature engineering approach is among the most significant improvements.  The algorithm captures crucial daily and weekly seasonality by extracting temporal characteristics including the hour of the day, day of the week, and weekend indications. It also encodes cyclical patterns using sine and cosine transforms.  Accurate forecasting with time series data requires the model or baseline to take use of recent past patterns, which is further made possible by the addition of lagged values for the target variable.
The per-AP feature normalization is another significant enhancement.  This makes sure that every access point is scaled separately, which keeps the model from being skewed by variations in magnitude and enables fair comparisons between APs with different traffic volumes.  The pipeline additionally maintains the data's chronological order by using a timewise train/validation/test split.  This method keeps data from leaking and simulates forecasting situations in the actual world, where forecasts are constantly produced based on data from the future.
 While the calculation of several error metrics—MSE, MAE, and MAPE—offers a thorough picture of performance, the addition of a basic lag-based baseline model offers a useful comparison.  Lastly, the findings are visualized across metrics and APs, which makes model evaluation clear and useful.
